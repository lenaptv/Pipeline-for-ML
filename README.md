# Пайплан для процесса машинного обучения
Предположим, что с определенной периодичностью (ежемесячно, ежеквартально и т.п.) у вас появляются новые данные, и появляется необходимость обновить прогноз, в том числе актуализировать ранее выбранную модель машинного обучения.

В таком случае было бы здорово автоматизировать данный процесс: предобработку данных, обнаружение категориальных признаков и их трансформацию при помощи OneHotEncoder(), разбивку датасета на подвыборки,  выбор модели и расчет метрик для каждой, финальный расчет метрик на тестовой выборке и сохранение вновь выбранной модели. Именно это и делает данная программа.

## Содержание

1. [Глава I](#глава-i) \
    1.1. [Цели](#Цели)
2. [Глава II](#глава-ii) \
    2.1. [Что делает данная программа](#Что-делает-данная-программа)
3. [Глава III](#глава-iii) \
    3.1. [Исследования](#Исследования)
4. [Глава IV](#глава-iv) \
    4.1. [Запуск и работа программы](#Запуск-и-работа-программы)
5. [Глава V](#глава-v) \
    5.1. [Запуск телеграм-бота](#Запуск-телеграм-бота)

   

## Глава I
### Цели

Данная программа автоматизирует процесс машинного обучения для следующего случая.
Рассмотрим пример, если бы для школы, обучающей ИТ-профессиям, было бы важно предсказать день недели, в котором был сделан определенный коммит в зависимости от следующих признаков: студента, номера лабораторной работы, часа и порядкового номера попытки.
При этом есть датасет со следующими данными: `uid` - ник студента, `labname` - название лабораторной работы, `numTrials` - порядковый номер попытки, `timestamp` - временная метка с датой и точным временем коммита.
Каждый месяц датасет существенно меняется в виду новой информации и требуется пересматривать подход к прогнозированию дня недели коммита и обновлять прогноз.

## Глава II
### Что делает данная программа

Основу данной программы представляет собой скрипт на языке Python `base_classes.py`, который содержит необходимые классы и методы.

Первые три кастомных класса-трансформера выполняют следующие задачи:
- Вытащить из колонки `timestamp` час и день недели (в виде цифры), а затем удалить ставший ненужным столбец `timestamp`.
- Обнаружить все категориальные признаки и выполнить их трансформацию при помощи OneHotEncoder(), а затем удалить ставшие ненужными первоначальные столбцы с категориальными признаками.
- Выделить в отдельный series столбец с целевой переменной.
- Выполнить разбивку датасета на подвыборки: тренировочную, валидационную и тестовую.

Следующий класс принимает на вход список экземпляров GridSearchCV и словарь, в котором ключами являются индексы из этого списка, а значениями – названия моделей.

И наконец, финальный класс выполняет 

1. Она принимает на вход список ингредиентов.
2. Она создает прогноз и выдает рейтинг с оценкой (`bad`, `so-so`, `great`) блюда, которое можно приготовить на основе списка переданных ингредиентов.
3. Она находит и выводит информацию обо всех питательных веществах (белках, жирах и т. д.) продуктов из списка, а также их объем в процентах от суточной нормы потребления. При этом в первую очередь программа выводит информацию по 3м основным нутриентам: белок, холестерин, витамин Д, а затем еще по 3м в порядке убывания  максимальной доли покрытия суточной потребности в соответствующем нутриенте.
4. Она отображает три рецепта блюд, в которых используется максимальное количество продуктов из списка, с указанием их оценки и ссылок, по которым пользователь сможет найти полную информацию.

Второй вариант работы программы (`nutritionist.py menu`) заключается в следующем:
1. Предлагает соответствующие рецепты для завтрака, обеда и ужина (с ссылками для более подробного описания).
2. При каждом запуске программы рецепты обновляются.
3. Кроме ссылок на предлагаемые рецепты программа выводит также основную информацию по рецепту: 
рейтинг, список необходимых ингредиентов и долю основных нутриентов от суточной нормы потребления.
4. Предлагаемые рецепты имеют высокий рейтинг (больше 4х из 5) и высокий уровень покрытия суточной потребности по пищевой ценности.

Кроме того, пользователь может запустить данную программу через соответсвующий телеграм-бот.

## Глава III
### Исследования

Детали проведенных исследований и расчетов содержатся в файле `research.ipynb`
Ниже коротко описана логика исследований и подготовки файлов для работы программы.

#### Разработка модели прогнозирования оценки сочетания ингредиентов (`bad`, `so-so`, `great`)

Чтобы программа могла предсказать оценку блюда, которое можно приготовить на основе списка переданных ингредиентов, необходима соответствующая модель машинного обучения. 

* Подготовка данных 

Для разработки модели прогонозирования за основу был взят датасет из Epicurious, подготовленный HugoDarwood  https://www.kaggle.com/datasets/hugodarwood/epirecipes

После предварительной обработки данного датасета были проведены следующие итерации в поисках оптимальной модели машинного обучения.

* Регрессия

 1. Для прогнозирования рейтинга (именно в формате десятичных дробей с точностью до тысячных долей) были опробованы различные регрессионные алгоритмы, ансамбли и их гиперпараметры, выбрано лучшее решение на основе gridsearch и кроссвалидации.
 2. Затем для лучшего алгоритма и ансамбля было оценено RMSE на тестовой подвыборке, который оказался в итоге примерно на уровне RMSE для наивного регрессора (наивный алгоритм в данном случае следующий: для всех наблюдений спрогнозирован средний рейтинг).

Что если прогнозировать не рейтинг, а класс как ближайшее целое число рейтинга?
Переходим к моделям классификации.

* Классификация

1. Для прогнозирования класса (как целого числа рейтинга) были опробованы различные алгоритмы, ансамбли и их гиперпараметры, выбрано лучшее решение на основе gridsearch и кроссвалидации.
2. Затем для лучшего алгоритма и ансамбля было рассчитано accuracy на тестовой подвыборке, но данная метрика снова оказалсь примерно на уровне accuracy для наивного алгоритма (наивный алгоритм в данном случае следующий: для всех наблюдений спрогнозирован самый популярный класс).
3. Что если прогнозировать не целое число рейтинга, а следующие классы: bad (0, 1) (невкусное), so-so (2, 3) (нормальное), great (4, 5) (вкусное)?
Для прогнозирования  данных классов вновь были опробованы различные алгоритмы, ансамбли и их гиперпараметры. Метрики данных моделей уже значительно выше, чем у моделей 2х предыдущих подходов и наивных алгоритмов.
4. Далее в качестве ключевой метрики для выбора модели из п. 3 выше определена precision по классу great, тк в работе данного продукта наиболее критичной ошибкой будет предсказать great для блюда с низким рейтингом по факту (тк в данном случае пользователь будет расстроен и может дать негативный отзыв о работе программы). 
Исходя из данной логики была выбрана модель классификации по классам (bad, so-so, great) на основе алгоритма RandomForestClassifier.

#### Подготовка датафреймов с информацией о пищевой ценности продуктов и подходящих рецептах

1. Для подготовки информации о пищевой ценности продуктов для отобранных ранее ингредиентов (в предобработанном первоначальном датасете, см. выше раздел "Подготовка данных")  был использован следующий API: https://fdc.nal.usda.gov/api-guide.html  
Затем все значения были конвертированы в % от суточной нормы потребления.
2. Для каждого рецепта из набора данных (предобработанный первоначальный датасете, см. выше раздел "Подготовка данных") с помощью парсинга была получена сссылка с сайта epicurious.com
Затем в файле с классами и методами работы основной программы `reсipies.py` в соответствующем методе задано условие, что, в перую очередь, выводится тот рецепт, в котором не требуется никаких других ингредиентов.  Далее идет тот, у которого появляется 1 доп. ингредиент. Далее – 2. Если рецепт нуждается в более, чем 5 доп. ингредиентах, то такой рецепт не выводится.     
3. Для второго варианта программы были отобраны (из предобработанного первоначального датасета с добавленными ссылками на рецепты) только рецепты,  подходящие для определенной категории приема пищи (завтрак, обед или ужин), только с высоким рейтингом (4 и выше) и наиболее питательные для данной категории приема пищи (с наибольшим покрытием суточной потребности по пищевой ценности). 


## Глава IV
### Запуск и работа программы

 - Для запуска программы скачайте файлы из данного репозитория, разархивируйте папку `data` целиком
 - Также подключите в в среде разработки библиотеки из файла requirements.txt
 - Откройт в среде разработки файл с классами и методами: `base_classes.py`
 - Затем запустите файл с запуском программы: `pipeline.py`
 В данном файле уже считывается датасет-пример, его можно поменять, но тк данная программа предобработки прописана для определенного случая, то новый датасет должен быть таким же по структуре как и файл из примера.
 А вот список моделей и соответствующих вариантов гиперпараметров для Greadsearch можно обновить по усмотрению.
 - В процессе работы программа попросит вас принять решение о выборе модели, ввести ее название и наилучшие параметры


